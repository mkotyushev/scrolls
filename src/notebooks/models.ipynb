{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from unittest.mock import patch\n",
    "\n",
    "from src.model.swin_transformer_v2_pseudo_3d import SwinTransformerV2Pseudo3d, map_pretrained_2d_to_pseudo_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2d = timm.create_model(\n",
    "    'swinv2_tiny_window8_256.ms_in1k', \n",
    "    features_only=True,\n",
    "    pretrained=True,\n",
    ")\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "y = model_2d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with patch('timm.models.swin_transformer_v2.SwinTransformerV2', SwinTransformerV2Pseudo3d):\n",
    "    model_pseudo_3d = timm.create_model(\n",
    "        'swinv2_tiny_window8_256.ms_in1k', \n",
    "        features_only=True,\n",
    "        pretrained=False,\n",
    "        window_size=(8, 8, 16),\n",
    "        img_size=(256, 256, 64),\n",
    "    )\n",
    "x = torch.randn(1, 3, 256, 256, 64)\n",
    "y = model_pseudo_3d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.proj.weight: torch.Size([96, 3, 4, 4]) -> torch.Size([96, 3, 4, 4, 4])\n",
      "patch_embed.proj.bias: torch.Size([96]) -> OK\n",
      "patch_embed.norm.weight: torch.Size([96]) -> OK\n",
      "patch_embed.norm.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.attn.logit_scale: torch.Size([3, 1, 1]) -> OK\n",
      "layers_0.blocks.0.attn.q_bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.attn.v_bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> torch.Size([512, 3])\n",
      "layers_0.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_0.blocks.0.attn.cpb_mlp.2.weight: torch.Size([3, 512]) -> OK\n",
      "layers_0.blocks.0.attn.qkv.weight: torch.Size([288, 96]) -> OK\n",
      "layers_0.blocks.0.attn.proj.weight: torch.Size([96, 96]) -> OK\n",
      "layers_0.blocks.0.attn.proj.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.norm1.weight: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.norm1.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.mlp.fc1.weight: torch.Size([384, 96]) -> OK\n",
      "layers_0.blocks.0.mlp.fc1.bias: torch.Size([384]) -> OK\n",
      "layers_0.blocks.0.mlp.fc2.weight: torch.Size([96, 384]) -> OK\n",
      "layers_0.blocks.0.mlp.fc2.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.norm2.weight: torch.Size([96]) -> OK\n",
      "layers_0.blocks.0.norm2.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.attn.logit_scale: torch.Size([3, 1, 1]) -> OK\n",
      "layers_0.blocks.1.attn.q_bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.attn.v_bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_0.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_0.blocks.1.attn.cpb_mlp.2.weight: torch.Size([3, 512]) -> OK\n",
      "layers_0.blocks.1.attn.qkv.weight: torch.Size([288, 96]) -> OK\n",
      "layers_0.blocks.1.attn.proj.weight: torch.Size([96, 96]) -> OK\n",
      "layers_0.blocks.1.attn.proj.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.norm1.weight: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.norm1.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.mlp.fc1.weight: torch.Size([384, 96]) -> OK\n",
      "layers_0.blocks.1.mlp.fc1.bias: torch.Size([384]) -> OK\n",
      "layers_0.blocks.1.mlp.fc2.weight: torch.Size([96, 384]) -> OK\n",
      "layers_0.blocks.1.mlp.fc2.bias: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.norm2.weight: torch.Size([96]) -> OK\n",
      "layers_0.blocks.1.norm2.bias: torch.Size([96]) -> OK\n",
      "layers_1.downsample.reduction.weight: torch.Size([192, 384]) -> OK\n",
      "layers_1.downsample.norm.weight: torch.Size([192]) -> OK\n",
      "layers_1.downsample.norm.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.attn.logit_scale: torch.Size([6, 1, 1]) -> OK\n",
      "layers_1.blocks.0.attn.q_bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.attn.v_bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_1.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_1.blocks.0.attn.cpb_mlp.2.weight: torch.Size([6, 512]) -> OK\n",
      "layers_1.blocks.0.attn.qkv.weight: torch.Size([576, 192]) -> OK\n",
      "layers_1.blocks.0.attn.proj.weight: torch.Size([192, 192]) -> OK\n",
      "layers_1.blocks.0.attn.proj.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.norm1.weight: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.norm1.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.mlp.fc1.weight: torch.Size([768, 192]) -> OK\n",
      "layers_1.blocks.0.mlp.fc1.bias: torch.Size([768]) -> OK\n",
      "layers_1.blocks.0.mlp.fc2.weight: torch.Size([192, 768]) -> OK\n",
      "layers_1.blocks.0.mlp.fc2.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.norm2.weight: torch.Size([192]) -> OK\n",
      "layers_1.blocks.0.norm2.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.attn.logit_scale: torch.Size([6, 1, 1]) -> OK\n",
      "layers_1.blocks.1.attn.q_bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.attn.v_bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_1.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_1.blocks.1.attn.cpb_mlp.2.weight: torch.Size([6, 512]) -> OK\n",
      "layers_1.blocks.1.attn.qkv.weight: torch.Size([576, 192]) -> OK\n",
      "layers_1.blocks.1.attn.proj.weight: torch.Size([192, 192]) -> OK\n",
      "layers_1.blocks.1.attn.proj.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.norm1.weight: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.norm1.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.mlp.fc1.weight: torch.Size([768, 192]) -> OK\n",
      "layers_1.blocks.1.mlp.fc1.bias: torch.Size([768]) -> OK\n",
      "layers_1.blocks.1.mlp.fc2.weight: torch.Size([192, 768]) -> OK\n",
      "layers_1.blocks.1.mlp.fc2.bias: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.norm2.weight: torch.Size([192]) -> OK\n",
      "layers_1.blocks.1.norm2.bias: torch.Size([192]) -> OK\n",
      "layers_2.downsample.reduction.weight: torch.Size([384, 768]) -> OK\n",
      "layers_2.downsample.norm.weight: torch.Size([384]) -> OK\n",
      "layers_2.downsample.norm.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.attn.logit_scale: torch.Size([12, 1, 1]) -> OK\n",
      "layers_2.blocks.0.attn.q_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.attn.v_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_2.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_2.blocks.0.attn.cpb_mlp.2.weight: torch.Size([12, 512]) -> OK\n",
      "layers_2.blocks.0.attn.qkv.weight: torch.Size([1152, 384]) -> OK\n",
      "layers_2.blocks.0.attn.proj.weight: torch.Size([384, 384]) -> OK\n",
      "layers_2.blocks.0.attn.proj.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.norm1.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.norm1.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.mlp.fc1.weight: torch.Size([1536, 384]) -> OK\n",
      "layers_2.blocks.0.mlp.fc1.bias: torch.Size([1536]) -> OK\n",
      "layers_2.blocks.0.mlp.fc2.weight: torch.Size([384, 1536]) -> OK\n",
      "layers_2.blocks.0.mlp.fc2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.norm2.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.0.norm2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.attn.logit_scale: torch.Size([12, 1, 1]) -> OK\n",
      "layers_2.blocks.1.attn.q_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.attn.v_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_2.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_2.blocks.1.attn.cpb_mlp.2.weight: torch.Size([12, 512]) -> OK\n",
      "layers_2.blocks.1.attn.qkv.weight: torch.Size([1152, 384]) -> OK\n",
      "layers_2.blocks.1.attn.proj.weight: torch.Size([384, 384]) -> OK\n",
      "layers_2.blocks.1.attn.proj.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.norm1.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.norm1.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.mlp.fc1.weight: torch.Size([1536, 384]) -> OK\n",
      "layers_2.blocks.1.mlp.fc1.bias: torch.Size([1536]) -> OK\n",
      "layers_2.blocks.1.mlp.fc2.weight: torch.Size([384, 1536]) -> OK\n",
      "layers_2.blocks.1.mlp.fc2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.norm2.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.1.norm2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.attn.logit_scale: torch.Size([12, 1, 1]) -> OK\n",
      "layers_2.blocks.2.attn.q_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.attn.v_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_2.blocks.2.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_2.blocks.2.attn.cpb_mlp.2.weight: torch.Size([12, 512]) -> OK\n",
      "layers_2.blocks.2.attn.qkv.weight: torch.Size([1152, 384]) -> OK\n",
      "layers_2.blocks.2.attn.proj.weight: torch.Size([384, 384]) -> OK\n",
      "layers_2.blocks.2.attn.proj.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.norm1.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.norm1.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.mlp.fc1.weight: torch.Size([1536, 384]) -> OK\n",
      "layers_2.blocks.2.mlp.fc1.bias: torch.Size([1536]) -> OK\n",
      "layers_2.blocks.2.mlp.fc2.weight: torch.Size([384, 1536]) -> OK\n",
      "layers_2.blocks.2.mlp.fc2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.norm2.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.2.norm2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.attn.logit_scale: torch.Size([12, 1, 1]) -> OK\n",
      "layers_2.blocks.3.attn.q_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.attn.v_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_2.blocks.3.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_2.blocks.3.attn.cpb_mlp.2.weight: torch.Size([12, 512]) -> OK\n",
      "layers_2.blocks.3.attn.qkv.weight: torch.Size([1152, 384]) -> OK\n",
      "layers_2.blocks.3.attn.proj.weight: torch.Size([384, 384]) -> OK\n",
      "layers_2.blocks.3.attn.proj.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.norm1.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.norm1.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.mlp.fc1.weight: torch.Size([1536, 384]) -> OK\n",
      "layers_2.blocks.3.mlp.fc1.bias: torch.Size([1536]) -> OK\n",
      "layers_2.blocks.3.mlp.fc2.weight: torch.Size([384, 1536]) -> OK\n",
      "layers_2.blocks.3.mlp.fc2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.norm2.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.3.norm2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.attn.logit_scale: torch.Size([12, 1, 1]) -> OK\n",
      "layers_2.blocks.4.attn.q_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.attn.v_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_2.blocks.4.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_2.blocks.4.attn.cpb_mlp.2.weight: torch.Size([12, 512]) -> OK\n",
      "layers_2.blocks.4.attn.qkv.weight: torch.Size([1152, 384]) -> OK\n",
      "layers_2.blocks.4.attn.proj.weight: torch.Size([384, 384]) -> OK\n",
      "layers_2.blocks.4.attn.proj.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.norm1.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.norm1.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.mlp.fc1.weight: torch.Size([1536, 384]) -> OK\n",
      "layers_2.blocks.4.mlp.fc1.bias: torch.Size([1536]) -> OK\n",
      "layers_2.blocks.4.mlp.fc2.weight: torch.Size([384, 1536]) -> OK\n",
      "layers_2.blocks.4.mlp.fc2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.norm2.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.4.norm2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.attn.logit_scale: torch.Size([12, 1, 1]) -> OK\n",
      "layers_2.blocks.5.attn.q_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.attn.v_bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_2.blocks.5.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_2.blocks.5.attn.cpb_mlp.2.weight: torch.Size([12, 512]) -> OK\n",
      "layers_2.blocks.5.attn.qkv.weight: torch.Size([1152, 384]) -> OK\n",
      "layers_2.blocks.5.attn.proj.weight: torch.Size([384, 384]) -> OK\n",
      "layers_2.blocks.5.attn.proj.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.norm1.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.norm1.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.mlp.fc1.weight: torch.Size([1536, 384]) -> OK\n",
      "layers_2.blocks.5.mlp.fc1.bias: torch.Size([1536]) -> OK\n",
      "layers_2.blocks.5.mlp.fc2.weight: torch.Size([384, 1536]) -> OK\n",
      "layers_2.blocks.5.mlp.fc2.bias: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.norm2.weight: torch.Size([384]) -> OK\n",
      "layers_2.blocks.5.norm2.bias: torch.Size([384]) -> OK\n",
      "layers_3.downsample.reduction.weight: torch.Size([768, 1536]) -> OK\n",
      "layers_3.downsample.norm.weight: torch.Size([768]) -> OK\n",
      "layers_3.downsample.norm.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.attn.logit_scale: torch.Size([24, 1, 1]) -> OK\n",
      "layers_3.blocks.0.attn.q_bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.attn.v_bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_3.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_3.blocks.0.attn.cpb_mlp.2.weight: torch.Size([24, 512]) -> OK\n",
      "layers_3.blocks.0.attn.qkv.weight: torch.Size([2304, 768]) -> OK\n",
      "layers_3.blocks.0.attn.proj.weight: torch.Size([768, 768]) -> OK\n",
      "layers_3.blocks.0.attn.proj.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.norm1.weight: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.norm1.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.mlp.fc1.weight: torch.Size([3072, 768]) -> OK\n",
      "layers_3.blocks.0.mlp.fc1.bias: torch.Size([3072]) -> OK\n",
      "layers_3.blocks.0.mlp.fc2.weight: torch.Size([768, 3072]) -> OK\n",
      "layers_3.blocks.0.mlp.fc2.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.norm2.weight: torch.Size([768]) -> OK\n",
      "layers_3.blocks.0.norm2.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.attn.logit_scale: torch.Size([24, 1, 1]) -> OK\n",
      "layers_3.blocks.1.attn.q_bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.attn.v_bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> OK\n",
      "layers_3.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512]) -> OK\n",
      "layers_3.blocks.1.attn.cpb_mlp.2.weight: torch.Size([24, 512]) -> OK\n",
      "layers_3.blocks.1.attn.qkv.weight: torch.Size([2304, 768]) -> OK\n",
      "layers_3.blocks.1.attn.proj.weight: torch.Size([768, 768]) -> OK\n",
      "layers_3.blocks.1.attn.proj.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.norm1.weight: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.norm1.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.mlp.fc1.weight: torch.Size([3072, 768]) -> OK\n",
      "layers_3.blocks.1.mlp.fc1.bias: torch.Size([3072]) -> OK\n",
      "layers_3.blocks.1.mlp.fc2.weight: torch.Size([768, 3072]) -> OK\n",
      "layers_3.blocks.1.mlp.fc2.bias: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.norm2.weight: torch.Size([768]) -> OK\n",
      "layers_3.blocks.1.norm2.bias: torch.Size([768]) -> OK\n"
     ]
    }
   ],
   "source": [
    "model_2d_state_dict = model_2d.state_dict()\n",
    "model_pseudo_3d_state_dict = model_pseudo_3d.state_dict()\n",
    "for key, value in model_2d_state_dict.items():\n",
    "    if key in model_pseudo_3d_state_dict:\n",
    "        if value.shape == model_pseudo_3d_state_dict[key].shape:\n",
    "            print(f'{key}: {value.shape} -> OK')\n",
    "        else:\n",
    "            print(f'{key}: {value.shape} -> {model_pseudo_3d_state_dict[key].shape}')\n",
    "    else:\n",
    "        print(f'{key}: {value.shape} -> NOT FOUND')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No-matches are `patch_embed.proj` (Conv2d -> Conv3d) and `layers.0.blocks.0.attn.cpb_mlp.0` (relative position bias mapping MLP for Z dim) layers' weights and biases, algthough biases shapes match. \n",
    "\n",
    "- Conv layer's weight: `torch.Size([96, 3, 4, 4]) -> torch.Size([96, 3, 4, 4, 4])`\n",
    "\n",
    "- MLP's weight: `torch.Size([512, 2]) -> torch.Size([512, 3])`\n",
    "\n",
    "For conv layer proposal is to repeat weights along 3rd dimension and scale them down by patch size along Z dim (4) and keep bias term intact. E. g. if the image is just repeated along Z dim, then the 3D patch embedding in such case will be equal to 2D patch embedding of non-repeated patch.\n",
    "\n",
    "For relative position bias proposal is to calculate weights for new dimention as mean of weights of previous two and keep the bias intact. No invariancy for that case.\n",
    "\n",
    "**Note**: it needs additional investigation whether low-rank of the obtained weights is a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.proj.weight: torch.Size([96, 3, 4, 4]) -> torch.Size([96, 3, 4, 4, 4])\n",
      "layers_0.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2]) -> torch.Size([512, 3])\n"
     ]
    }
   ],
   "source": [
    "model = map_pretrained_2d_to_pseudo_3d(model_2d, model_pseudo_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 64, 64, 96]),\n",
       " torch.Size([1, 32, 32, 192]),\n",
       " torch.Size([1, 16, 16, 384]),\n",
       " torch.Size([1, 8, 8, 768])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(1, 3, 256, 256, 64)\n",
    "y = model(x)\n",
    "[y_.shape for y_ in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.layers.format import nhwc_to, Format\n",
    "from torch import nn\n",
    "\n",
    "class FeatureExtractorWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.output_stride = 32\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [nhwc_to(y, Format('NCHW')) for y in self.model(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_layers(model):\n",
    "    return len([key for key in model if 'layers' in key])\n",
    "get_num_layers(model), get_num_layers(FeatureExtractorWrapper(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 192, 384, 768), (96, 192, 384, 768))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature_channels(model, input_shape):\n",
    "    is_training = model.training\n",
    "    model.eval()\n",
    "    x = torch.randn(1, *input_shape)\n",
    "    y = model(x)\n",
    "    if isinstance(model, FeatureExtractorWrapper):\n",
    "        channel_index = 1\n",
    "    else:\n",
    "        channel_index = 3\n",
    "    result = tuple(y_.shape[channel_index] for y_ in y)\n",
    "    model.train(is_training)\n",
    "    return result\n",
    "get_feature_channels(model, (3, 256, 256, 64)), \\\n",
    "get_feature_channels(FeatureExtractorWrapper(model), (3, 256, 256, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Union\n",
    "try:\n",
    "    from inplace_abn import InPlaceABN\n",
    "except ImportError:\n",
    "    InPlaceABN = None\n",
    "\n",
    "\n",
    "def initialize_decoder(module):\n",
    "    for m in module.modules():\n",
    "\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def initialize_head(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding=0,\n",
    "        stride=1,\n",
    "        use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "\n",
    "class ArgMax(nn.Module):\n",
    "    def __init__(self, dim=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.argmax(x, dim=self.dim)\n",
    "\n",
    "\n",
    "class Clamp(nn.Module):\n",
    "    def __init__(self, min=0, max=1):\n",
    "        super().__init__()\n",
    "        self.min, self.max = min, max\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(x, self.min, self.max)\n",
    "\n",
    "\n",
    "class Activation(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None or name == \"identity\":\n",
    "            self.activation = nn.Identity(**params)\n",
    "        elif name == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif name == \"softmax2d\":\n",
    "            self.activation = nn.Softmax(dim=1, **params)\n",
    "        elif name == \"softmax\":\n",
    "            self.activation = nn.Softmax(**params)\n",
    "        elif name == \"logsoftmax\":\n",
    "            self.activation = nn.LogSoftmax(**params)\n",
    "        elif name == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        elif name == \"argmax\":\n",
    "            self.activation = ArgMax(**params)\n",
    "        elif name == \"argmax2d\":\n",
    "            self.activation = ArgMax(dim=1, **params)\n",
    "        elif name == \"clamp\":\n",
    "            self.activation = Clamp(**params)\n",
    "        elif callable(name):\n",
    "            self.activation = name(**params)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Activation should be callable/sigmoid/softmax/logsoftmax/tanh/\"\n",
    "                f\"argmax/argmax2d/clamp/None; got {name}\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x)\n",
    "\n",
    "class SCSEModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == \"scse\":\n",
    "            self.attention = SCSEModule(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(conv2d, upsampling, activation)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n",
    "        if pooling not in (\"max\", \"avg\"):\n",
    "            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n",
    "        pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)\n",
    "        flatten = nn.Flatten()\n",
    "        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n",
    "        linear = nn.Linear(in_channels, classes, bias=True)\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(pool, flatten, dropout, linear, activation)\n",
    "\n",
    "class SegmentationModel(torch.nn.Module):\n",
    "    def initialize(self):\n",
    "        initialize_decoder(self.decoder)\n",
    "        initialize_head(self.segmentation_head)\n",
    "        if self.classification_head is not None:\n",
    "            initialize_head(self.classification_head)\n",
    "\n",
    "    def check_input_shape(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        output_stride = self.encoder.output_stride\n",
    "        if h % output_stride != 0 or w % output_stride != 0:\n",
    "            new_h = (h // output_stride + 1) * output_stride if h % output_stride != 0 else h\n",
    "            new_w = (w // output_stride + 1) * output_stride if w % output_stride != 0 else w\n",
    "            raise RuntimeError(\n",
    "                f\"Wrong input shape height={h}, width={w}. Expected image height and width \"\n",
    "                f\"divisible by {output_stride}. Consider pad your images to shape ({new_h}, {new_w}).\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "\n",
    "        self.check_input_shape(x)\n",
    "\n",
    "        features = self.encoder(x)\n",
    "        decoder_output = self.decoder(*features)\n",
    "\n",
    "        masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "        if self.classification_head is not None:\n",
    "            labels = self.classification_head(features[-1])\n",
    "            return masks, labels\n",
    "\n",
    "        return masks\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x):\n",
    "        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n",
    "\n",
    "        Args:\n",
    "            x: 4D torch tensor with shape (batch_size, channels, height, width)\n",
    "\n",
    "        Return:\n",
    "            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "\n",
    "        x = self.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention1 = Attention(attention_type, in_channels=in_channels + skip_channels)\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.attention2 = Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CenterBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
    "        conv1 = Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        super().__init__(conv1, conv2)\n",
    "\n",
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        decoder_channels,\n",
    "        n_blocks=5,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "        center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "        \n",
    "        if center:\n",
    "            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
    "        blocks = [\n",
    "            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
    "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, *features):\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skips = features[1:]\n",
    "\n",
    "        x = self.center(head)\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = decoder_block(x, skip)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Unet(SegmentationModel):\n",
    "    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder*\n",
    "    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n",
    "    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*\n",
    "    for fusing decoder blocks with skip connections.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n",
    "            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n",
    "            Available options are **True, False, \"inplace\"**\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "                **callable** and **None**.\n",
    "            Default is **None**\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        encoder_channels,\n",
    "        decoder_use_batchnorm: bool = True,\n",
    "        decoder_channels: List[int] | str = (256, 128, 64),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        encoder_channels = list(encoder_channels)\n",
    "        decoder_channels = list(decoder_channels)\n",
    "        if decoder_channels == 'same':\n",
    "            decoder_channels = encoder_channels\n",
    "\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=len(decoder_channels),\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=False,\n",
    "            attention_type=decoder_attention_type,\n",
    "        )\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(in_channels=self.encoder.out_channels[-1], **aux_params)\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 384, 192, 96)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_channels(FeatureExtractorWrapper(model), input_shape=(3, 256, 256, 64))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet(\n",
    "    encoder=FeatureExtractorWrapper(model),\n",
    "    input_shape=get_feature_channels(model, input_shape=(3, 256, 256, 64)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): FeatureExtractorWrapper(\n",
       "    (model): FeatureListNet(\n",
       "      (patch_embed): PatchEmbedPseudo3d(\n",
       "        (proj): Conv3d(3, 96, kernel_size=(4, 4, 4), stride=(4, 4, 4))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers_0): SwinTransformerV2StagePseudo3d(\n",
       "        (downsample): Identity()\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttentionPseudo3d(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=3, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.009)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.009)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_1): SwinTransformerV2StagePseudo3d(\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.018)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.018)\n",
       "          )\n",
       "          (1): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.027)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.027)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_2): SwinTransformerV2StagePseudo3d(\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.036)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.036)\n",
       "          )\n",
       "          (1): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.045)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.045)\n",
       "          )\n",
       "          (2): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.055)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.055)\n",
       "          )\n",
       "          (3): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.064)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.064)\n",
       "          )\n",
       "          (4): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.073)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.073)\n",
       "          )\n",
       "          (5): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.082)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.082)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_3): SwinTransformerV2StagePseudo3d(\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.091)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.091)\n",
       "          )\n",
       "          (1): SwinTransformerV2BlockPseudo3d(\n",
       "            (attn): WindowAttention(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "              )\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path1): DropPath(drop_prob=0.100)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (drop_path2): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1152, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(448, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256, 64)\n",
    "y = unet(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrolls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
